---
created: 2024-08-31T11:48:59-04:00
modified: 2024-09-01T15:40:03-04:00
---
![[{{formattingIssues}}]]
# Longtermism

![[450px-Longtermism2.jpg]]

**Longtermism** is a moral framework advocating ideas around the prioritization of potential futures as a moral priority. It has both advocates and critics.

## Advocacy
[[Pages/Effective altruism|Effective altruist]] founder [[Pages/William MacAskill|William MacAskill]]^<ref>[https://forum.effectivealtruism.org/posts/qZyshHCNkjs3TvSem/longtermism 'Longtermism']</ref> coined the term "longtermism" in 2019 as a moral framework that suggests positively influencing should be a key moral priority in our time.

## Criticism

### Summary
Since 2021, [[Pages/existential risk|existential risk]] researcher [[Pages/Phil Torres|Phil Torres]] has developed a critique of this framework, consolidating many of his pre-existing criticisms of futurist circles in his essay 'Against Longtermism'^[https://aeon.co/essays/why-longtermism-is-the-worlds-most-dangerous-secular-credo] Against longtermism

Torres, who describes himself as a "former longtermist"^<ref name="essay"/> and is a published futurist^<ref>[https://www.amazon.com/Morality-Foresight-Human-Flourishing-Introduction/dp/1634311426/ref=sr_1_1?dchild=1&keywords=morality+foresight+torres&qid=1625225082&sr=8-1 Morality, Foresight, and Human Flourishing: An Introduction to Existential Risks]</ref> is concerned about the increased following of [[Pages/Nick Bostrom|Nick Bostrom]] and other influential figures such as [[Pages/Elon Musk|Elon Musk]] in recent years.

### Climate Change
Requestioning [[Pages/climate change|climate change]]^<ref name="essay">[https://www.currentaffairs.org/2021/07/the-dangerous-ideas-of-longtermism-and-existential-risk The Dangerous Ideas of “Longtermism” and “Existential Risk]</ref> as an existential risk by [[Pages/Silicon Valley|Silicon Valley]] tech entrepreneurs is attributed to an adherence to the longtermist ideology.

### Value of Future Persons
Torres is critical of the idea that an individual premature death of a great thinker measured in "potential" can be scaled up to the totality of humanity itself into the future. Such calculations may quickly end up grossly valuing a huge number of future human lives potentially over existing lives.^<ref name="essay"/>

Similar exponential calculations can be applied to persons with great potential who fail to achieve it.

This is described as:
<blockquote>This is the central dogma of longtermism: nothing matters more, ethically speaking, than fulfilling our potential as a species of ‘Earth-originating intelligent life’. It matters so much that longtermists have even coined the scary-sounding term ‘existential risk’ for any possibility of our potential being destroyed, and ‘existential catastrophe’ for any event that actually destroys this potential.</blockquote>

### Futurism
Torres argues the argument that technological developments are "morally neutral" as a defense is dangerous, comparing it to the NRA slogan "Guns don’t kill people, people kill people."

Transhumanism and futurism in this context are characterized as overly potential-focused on topics such as expansionist [[Pages/space exploration|space exploration]] or individualistic human enhancement projects.

Post-human scenarios with huge simulations of people allow the future value of civilization to reach even further levels of irrefutable enormity.

### Utilitarianism
[[Pages/Utilitarianism|Utilitarianism]] and contemporary [[Pages/effective altruism|effective altruism]] are criticized on familiar grounds of hedonism, the [[Pages/repugnant conclusion|repugnant conclusion]] of population and happiness, and being focused on people as containers of value rather than having intrinsic worth.

### Inequality
It is argued that such calculations about personal potential and future civilization can devalue, for example, people in the [global south](https://en.wikipedia.org/wiki/Global_North_and_Global_South(global south)) critically threatened by climate change as irrelevant in a trillion-year cosmic context.

Futurist researcher Nick Beckstead argues that people in rich countries have inherently more value than those in poor countries:^[https://www.proquest.com/docview/1442191960?fromopenview=tr%2522a%2520long%2520period%25E2%2580%2594perhaps%2520tens%2520of%2520thousands%2520of%2520years%25E2%2580%2594during%2520which%2520human%2520civilisation,%2520perhaps%2520with%2520the%2520aid%2520of%2520improved%2520cognitive%2520ability,%2520dedicates%2520itself%2520to%2520working%2520out%2520what%2520is%2520ultimately%2520of%2520value%2522ue]
<blockquote>Saving lives in poor countries may have significantly smaller ripple effects than saving and improving lives in rich countries. Why? Richer countries have substantially more innovation, and their workers are much more economically productive. [Consequently,] it now seems more plausible to me that saving a life in a rich country is substantially more important than saving a life in a poor country, other things being equal.</blockquote>

## Dangerous Conclusions
Bostrom has characterized calamities such as AIDS and the Holocaust as 'mere ripples' in the context of existential risk.

Torres is concerned that this attitude of the "greater good" is a common theme in past atrocities, such as in wartime, under totalitarian regimes, or for nebulous "national security" pretexts. Indeed, Bostrom has advocated for a global surveillance network^[https://www.nickbostrom.com/papers/vulnerable.pdf] to prevent an [[Pages/Moore's Law of Mad Science|omnicidal terrorist attack]] destroying civilization and in favor of interventionist wars in the defense of perceived existential threats.

Mathematical statistician Olle Häggström says:^[https://oxford.universitypressscholarship.com/view/10.1093/acprof:oso/9780198723547.001.0001/acprof-9780198723547]
<blockquote>Imagine a situation where the head of the CIA explains to the US president that they have credible evidence that somewhere in Germany, there is a lunatic who is working on a doomsday weapon and intends to use it to wipe out humanity, and that this lunatic has a one-in-a-million chance of succeeding. They have no further information on the identity or whereabouts of this lunatic. If the president has taken Bostrom’s argument to heart, and if he knows how to do the arithmetic, he may conclude that it is worthwhile conducting a full-scale nuclear assault on Germany to kill every single person within its borders.</blockquote>

Fundamentally, Torres believes technology is far more likely to cause our extinction than to save us from it.

## In Fiction
* [[Pages/Soylent Green|Soylent Green]], while not featuring [[Pages/life extension|life extension]], features [[Pages/oligarchic transhumanism|inequality]], pollution, and overpopulation in a classic dystopian future scenario.^<ref>[https://en.wikipedia.org/wiki/Soylent_Green Wikipedia:Soylent Green]</ref>
* [[Pages/Inferno|Inferno]] features a nominally transhumanist villain who believes in the myth of human overpopulation to the point he believes the population is due to reach 32 billion.^<ref>[http://www.rationalargumentator.com/index/blog/2016/11/inferno-overpopulation-myth/ “Inferno” and the Overpopulation Myth – Article by Jonathan Newman]</ref>

> [!abstract] Related Links
> - [[Pages/Silicon Valley (TV show)|Silicon Valley (TV show)]]
> - [[Pages/The Californian Ideology|The Californian Ideology]]

> [!abstract] External Links
> - [Silicon Valley](https://en.wikipedia.org/wiki/Silicon_Valley)
> - [The Californian Ideology](https://en.wikipedia.org/wiki/The_Californian_Ideology)

> [!abstract] Categories
> [[Categories/Technology|Technology]] [[Categories/Emerging technologies|Emerging technologies]] [[Categories/Libertarian transhumanism|Libertarian transhumanism]] [[Categories/Opposition to transhumanism|Opposition to transhumanism]] [[Categories/Places|Places]]